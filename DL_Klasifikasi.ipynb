{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of DL NLP Klasifikasi.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXVsYvs48OAw"
      },
      "source": [
        "**Nama : Fenny Feronika Maharani**\n",
        "\n",
        "**NIM  : 1703015**\n",
        "\n",
        "**Tugas Klasifikasi dengan Deep Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixPUdA3Zk9i6"
      },
      "source": [
        "Library yang digunakan:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFtPqwwybdW_"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.models import load_model\n",
        "from keras import optimizers\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random as rn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0ESfav8EcUp"
      },
      "source": [
        "**1. LOAD DATA SET**\n",
        "\n",
        "  Karena format file dari data set merupakan .txt maka kita menggunakan fungsi open biasa. Dengan melakukan pemisahan antara text dan label yang dipisahkan oleh tanda \"==>\". Setiap pasangan text dan label dipisahkan dengan enter atau \"\\n\". Dalam data set sendiri selalu terdapat whitespace di awal dan akhir text dan label maka whitespace itu dapat dihilangkan menggunakan .strip()(untuk menghilangkan whitespace di awal dan akhir)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m0jxPhVWT0z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "25340acb-31af-4b4c-dd66-0c3d9277756b"
      },
      "source": [
        "namaFile = \"/content/drive/My Drive/KULIAH/smt6/NLP/KLASIFIKASI/dataset_klasifikasi_review_restoran.txt\"\n",
        "with open(namaFile, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "data = []\n",
        "label = []\n",
        "for line in lines:\n",
        "     txt = line.split('==>')\n",
        "     if len(txt) != 1 :\n",
        "         data.append(txt[0].strip())\n",
        "         label.append(txt[1].strip())\n",
        "\n",
        "print(data[:5])\n",
        "print(label[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Tempat ini mengimingi makan steak dan daging dengan harga murah . Namun kenyataannya tidak murah jika dibandingkan dengan kualitas dagingnya yang tidak enak . Dagingnya keras dan rasanya hambar .', \"Sudah lama sekali tidak mengunjungi resto yang satu ini . Tahu goreng berbumbunya enak . Gurame ma ' keke nya manis pedas kurang berapa enak . Nasi ulamnya komplit dan murah .\", 'Kunjungan ke restoran ini sangat mengecewakan karena dua menu yg saya pesan tidak tersedia seharusnya saat menyajikan menu hal ini disampaikan ke pembeli', 'Pertama kali nyobain makan disini , karena reviewnya sob buntutnya terkenal enak banget . Padahal gak terlalu doyan sop buntut sih sebenernya , tapi mumpung lagi di bandung ya udah cobain mampir aja . Kayanya enak makan sop hangat di udara dingin bandung . Tempatnya rumahan banget , homey dan nyaman . Meski kita datang gak pas jam makan , tapi tetep rame pengunjung .', 'Saya bersama keluarga makan di dapur dahapati anggrek . Semuanya baik2 saja , dari rasa makanan hingga kecepatan pelayanan . Namun pada saat selesai membayar , pelayan bulak balik melihat kami dan tiba2 lampu tempat kami dipadamkan . Padahal masih ada waktu 1 jam lagi sampai tutup . Kami menegur pelayan tersebut dan pelayan tersebut mengatakan gardu listrik mati ,']\n",
            "['NEG', 'POS', 'NEG', 'POS', 'NETRAL']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gq-bp4sEzvs"
      },
      "source": [
        "**2. KONVERSI LABEL MENJADI REPRESENTASI TENSOR**\n",
        "\n",
        "Konversi label dari \"NEG\", \"NETRAL\", \"POS\" menjadi representasi tensor.\n",
        "\n",
        "Hasilnya adalah label tensor 2D dengan shape (992, 3), karena ada 992 instance dengan 3 nilai label yang mungkin (negatif, netral, positif)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3cMJmRc9wfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "58227ca2-5563-4225-bc23-22a0fe4735ae"
      },
      "source": [
        "code = np.array(label)\n",
        "label_encoder = LabelEncoder()\n",
        "vec = label_encoder.fit_transform(code)\n",
        "label = to_categorical(vec)\n",
        "print(label.shape)\n",
        "print(label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(992, 3)\n",
            "[[1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " ...\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSKrXJYTFHS-"
      },
      "source": [
        "**3. SPLIT DATASET**\n",
        "\n",
        "Split data menjadi train dan test menggunakan scikit learn, 80% menjadi data train, 20% menjadi data test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrwwkclwW6No"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data, label,\n",
        "                                                   test_size=0.2,\n",
        "                                                   random_state=123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amIjT9MEFONX"
      },
      "source": [
        "**4. KONVERSI DATA TEKS MENJADI TF-IDF DAN TENSOR**\n",
        "\n",
        "Hasilnya adalah tensor 2D (793, 5646) untuk data train dan tensor 2D (119, 5646) untuk data test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUGDRqkgWoKi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "a38765c6-b596-4fa5-a863-815b4d123f7c"
      },
      "source": [
        "#konversi teks ke tfidf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "#fit hanya berdasarkan data train\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "#konversi train\n",
        "seq_x_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_enc_train = tokenizer.sequences_to_matrix(seq_x_train,mode=\"tfidf\")\n",
        "#konversi test\n",
        "seq_x_test  = tokenizer.texts_to_sequences(X_test)\n",
        "X_enc_test  = tokenizer.sequences_to_matrix(seq_x_test,mode=\"tfidf\")\n",
        "\n",
        "print(X_enc_train.shape)\n",
        "print(X_enc_test.shape)\n",
        "print(X_enc_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(793, 5646)\n",
            "(199, 5646)\n",
            "[[0.         1.50791899 1.71312898 ... 0.         0.         0.        ]\n",
            " [0.         0.         1.01180157 ... 0.         0.         0.        ]\n",
            " [0.         0.89060125 0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         1.50791899 1.01180157 ... 0.         0.         5.98519493]\n",
            " [0.         0.         1.71312898 ... 0.         0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIayBf93Facg"
      },
      "source": [
        "**5. Persiapan Model**\n",
        "\n",
        "Sebelum melakukan Training kita harus mempersiapkan dahulu model\n",
        "\n",
        "Ada empat layer: layer pertama adalah layer input hasil encode tf-idf sebelumnya: 5646 fitur. Mengapa  input_shape tidak menggunakan sample dimension atau sample axis  seperti (793, 5646)? karena jumlah samples tidak penting didefinisikan dalam layer input. Dengan teknik mini-batch,  sample dapat  diproses sedikit demi sedikit, jadi jumlahnya bisa berbeda-beda.  \n",
        "\n",
        "Activation softmax digunakan karena jumlah label ada 3 (negatif, netral dan positif). Jika jumlah label dua (binary classification) maka dapat digunakan activation sigmoid.   Setelah layer didefinisikan, maka layer dapat dicompile. Loss  categorical_crossentropy  dipilih karena terdapat tiga kelas,  sedangkan jika untuk binary class dapat digunakan  binary_crossentropy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIBsnPRWraRt"
      },
      "source": [
        "# Proses  mencari hyperparameter terbaik, dengan percobaan mengganti:\n",
        "\n",
        "1. Jumlah neuron\n",
        "\n",
        "    Percobaan mengganti jumlah neuron pada layer tidak mempengaruhi waktu training, dari seluruh percobaan dimulai dari \n",
        "    \n",
        "    a. Layer peryama 32 dan layer kedua 4\n",
        "\n",
        "    b. Layer peryama 16 dan layer kedua 8\n",
        "\n",
        "    c. Layer peryama 64 dan layer kedua 2\n",
        "\n",
        "    d. Layer peryama 8, layer kedua 6 dan layer ketiga 4\n",
        "\n",
        "* Akurasi tertinggi didapat oleh poin c\n",
        "* Grafik yang mendekati good fit dihasilkan oleh poin a"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0t3WWNQru5B"
      },
      "source": [
        "_,jum_fitur = X_enc_train.shape\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(32,activation='relu',input_shape=(jum_fitur,)))\n",
        "model.add(layers.Dense(4,activation='relu'))\n",
        "model.add(layers.Dense(3,activation='softmax'))  #karena kelasnya ada 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKRodUqytyfK"
      },
      "source": [
        "2. Fungsi Aktivasi\n",
        "\n",
        "    Percobaan mengganti fugsi aktivasi dari relu menjadi tanh berdampak pada grafik loss jika dibandingkan dengan relu, tanh membuat grafik menjadi overfit namun akurasi meningkat"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYSHJTzot6GS"
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(32, activation='tanh',input_shape=(jum_fitur,)))\n",
        "model.add(layers.Dense(4, activation='tanh'))\n",
        "model.add(layers.Dense(3, activation='softmax')) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKmrG1Gkrw9N"
      },
      "source": [
        "  3. Optimizer\n",
        "\n",
        "    Dari percobaan mengganti optimizer Adam, Adagrad, Adamax, SGD menghasilkan:\n",
        "*   Akurasi tertinggi didapat oleh opt Adam \n",
        "*   Grafik yang mendekati good fit didapat oleh opt Adam\n",
        "\n",
        "![Grafik Loss Adam](https://drive.google.com/file/d/11BgW8rfibs4KV4nmYc2zUIewkewqxZMZ/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqzN9y6EXHkc"
      },
      "source": [
        "model.compile(optimizer=\"adam\",\n",
        "             loss='categorical_crossentropy',\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBRW_p3StYj8"
      },
      "source": [
        "4. Learning Rate\n",
        "\n",
        "    Percobaan mengganti learning rate pada opt adam menjadi 0,01 dan 0,0001 mengakibatkan turunnya akurasi dan grafik menjadi overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5jsQCPRtdNh"
      },
      "source": [
        "opt = optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer= opt,\n",
        "             loss='categorical_crossentropy',\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOm5gOdfr8ic"
      },
      "source": [
        "Pada neural network, nilai random digunakan untuk inisialisasi bobot dan split batch. Ini menyebabkan konfigurasi yang sama dapat memberikan hasil berbeda setiap dijalankan. Untuk mencegahnya maka di perlukan kode seperti dibawah ini"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_snuJC2ehI4l"
      },
      "source": [
        "tf.random.set_seed(123)\n",
        "np.random.RandomState(123)  \n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "np.random.seed(123)\n",
        "rn.seed(123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UcIlSbwIKiu"
      },
      "source": [
        "**6. PROSES TRAINING**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-61EQ5MytLyA"
      },
      "source": [
        "\n",
        "Proses  mencari hyperparameter terbaik, selain dengan percobaan diatas dapat juga mengganti:\n",
        "5. Jumlah epoch\n",
        "\n",
        "    Dengan menambah jumlah epoch dari 3 menjadi 10, 20, 30, dan 50. Akurasi yang didapat memang semakin tinggi, namun dilihat dari grafik epoch vs loss terjadi overfitting. Jumlah epoch yang terlalu banyak membuat proses train terlalu lama dan model terlalu overfit terhadap data validasi, tetapi jika terlalu sedikit, model mungkin saja belum konvergen. Maka lebih baik training dilakukan dengan menggunakan early stopping. Keras menyediakan fasilitas callback yang dapat dimanfaatkan untuk menghentikan proses training jika sudah memenuhi kriteria tertentu.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3zZMkv0aJBT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "c8ea3e23-6c3d-4314-edc5-35726bdb204c"
      },
      "source": [
        "history = model.fit(X_enc_train,y_train,\n",
        "                   epochs=20, batch_size=2, \n",
        "                   validation_split=0.2)\n",
        "results = model.evaluate(X_enc_test, y_test) \n",
        "print(\"Hasil  [loss,acc] untuk data test:\")\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 634 samples, validate on 159 samples\n",
            "Epoch 1/20\n",
            "634/634 [==============================] - 1s 2ms/step - loss: 0.0153 - accuracy: 0.9937 - val_loss: 0.7198 - val_accuracy: 0.7547\n",
            "Epoch 2/20\n",
            "634/634 [==============================] - 1s 2ms/step - loss: 0.0109 - accuracy: 0.9953 - val_loss: 0.7526 - val_accuracy: 0.7484\n",
            "Epoch 3/20\n",
            "634/634 [==============================] - 1s 2ms/step - loss: 0.0081 - accuracy: 0.9968 - val_loss: 0.7742 - val_accuracy: 0.7421\n",
            "Epoch 4/20\n",
            "634/634 [==============================] - 1s 2ms/step - loss: 0.0070 - accuracy: 0.9968 - val_loss: 0.7876 - val_accuracy: 0.7421\n",
            "Epoch 5/20\n",
            "634/634 [==============================] - 1s 2ms/step - loss: 0.0064 - accuracy: 0.9968 - val_loss: 0.8033 - val_accuracy: 0.7421\n",
            "Epoch 6/20\n",
            "634/634 [==============================] - 1s 2ms/step - loss: 0.0060 - accuracy: 0.9968 - val_loss: 0.8205 - val_accuracy: 0.7484\n",
            "Epoch 7/20\n",
            "634/634 [==============================] - 1s 1ms/step - loss: 0.0057 - accuracy: 0.9968 - val_loss: 0.8301 - val_accuracy: 0.7484\n",
            "Epoch 8/20\n",
            "634/634 [==============================] - 1s 2ms/step - loss: 0.0055 - accuracy: 0.9968 - val_loss: 0.8446 - val_accuracy: 0.7484\n",
            "Epoch 9/20\n",
            "634/634 [==============================] - 1s 2ms/step - loss: 0.0053 - accuracy: 0.9968 - val_loss: 0.8639 - val_accuracy: 0.7296\n",
            "Epoch 10/20\n",
            "634/634 [==============================] - 1s 2ms/step - loss: 0.0047 - accuracy: 0.9984 - val_loss: 0.9321 - val_accuracy: 0.7296\n",
            "Epoch 11/20\n",
            "634/634 [==============================] - 1s 2ms/step - loss: 0.0041 - accuracy: 0.9984 - val_loss: 0.9218 - val_accuracy: 0.7358\n",
            "Epoch 12/20\n",
            "634/634 [==============================] - 1s 2ms/step - loss: 0.0038 - accuracy: 0.9984 - val_loss: 0.9627 - val_accuracy: 0.7358\n",
            "Epoch 13/20\n",
            "634/634 [==============================] - 1s 1ms/step - loss: 0.0057 - accuracy: 0.9968 - val_loss: 0.9158 - val_accuracy: 0.7233\n",
            "Epoch 14/20\n",
            "634/634 [==============================] - 1s 2ms/step - loss: 0.0035 - accuracy: 0.9984 - val_loss: 0.9391 - val_accuracy: 0.7233\n",
            "Epoch 15/20\n",
            "634/634 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9984 - val_loss: 0.9551 - val_accuracy: 0.7233\n",
            "Epoch 16/20\n",
            "634/634 [==============================] - 1s 1ms/step - loss: 0.0033 - accuracy: 0.9984 - val_loss: 0.9675 - val_accuracy: 0.7233\n",
            "Epoch 17/20\n",
            "634/634 [==============================] - 1s 1ms/step - loss: 0.0033 - accuracy: 0.9984 - val_loss: 0.9818 - val_accuracy: 0.7233\n",
            "Epoch 18/20\n",
            "634/634 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9984 - val_loss: 0.9933 - val_accuracy: 0.7233\n",
            "Epoch 19/20\n",
            "634/634 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9984 - val_loss: 1.0016 - val_accuracy: 0.7296\n",
            "Epoch 20/20\n",
            "634/634 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9984 - val_loss: 1.0158 - val_accuracy: 0.7296\n",
            "199/199 [==============================] - 0s 67us/step\n",
            "Hasil  [loss,acc] untuk data test:\n",
            "[1.755170079631422, 0.6733668446540833]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJP_5ap9GDcm"
      },
      "source": [
        "Menggunakan Early Stopping dengan syarat dalam proses train jika nilai loss pada data validation (val_loss) tidak berubah selama 3 epoch (patience=3). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl63Sdas-zIV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "78902766-30a2-495f-aee9-084d28b18bbd"
      },
      "source": [
        "esCallBack = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                              min_delta=0,\n",
        "                                              patience=3,\n",
        "                                              verbose=0, mode='auto')\n",
        "history = model.fit(X_enc_train,y_train,\n",
        "                  epochs=20, batch_size=2,\n",
        "                  validation_split=0.2, callbacks=[esCallBack])\n",
        "\n",
        "results = model.evaluate(X_enc_test, y_test) \n",
        "print(\"Hasil  [loss,acc] untuk data test:\")\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 634 samples, validate on 159 samples\n",
            "Epoch 1/20\n",
            "634/634 [==============================] - 1s 2ms/step - loss: 0.9098 - accuracy: 0.6293 - val_loss: 0.6579 - val_accuracy: 0.7610\n",
            "Epoch 2/20\n",
            "634/634 [==============================] - 1s 2ms/step - loss: 0.3559 - accuracy: 0.8817 - val_loss: 0.5689 - val_accuracy: 0.7736\n",
            "Epoch 3/20\n",
            "634/634 [==============================] - 1s 2ms/step - loss: 0.1212 - accuracy: 0.9669 - val_loss: 0.7103 - val_accuracy: 0.7421\n",
            "Epoch 4/20\n",
            "634/634 [==============================] - 1s 2ms/step - loss: 0.0442 - accuracy: 0.9905 - val_loss: 0.7102 - val_accuracy: 0.7233\n",
            "Epoch 5/20\n",
            "634/634 [==============================] - 1s 2ms/step - loss: 0.0251 - accuracy: 0.9921 - val_loss: 0.7281 - val_accuracy: 0.7421\n",
            "199/199 [==============================] - 0s 76us/step\n",
            "Hasil  [loss,acc] untuk data test:\n",
            "[1.185609549134221, 0.6934673190116882]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVIIxFduqOBz"
      },
      "source": [
        "6. Batch size\n",
        "\n",
        "    Dengan menambah batch size proses training menjadi lebih cepat, akurasi yang didapat malah menurun.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzMiJOACqpxl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "35adb0a7-6299-4920-8715-c13ff2570571"
      },
      "source": [
        "esCallBack = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                              min_delta=0,\n",
        "                                              patience=3,\n",
        "                                              verbose=0, mode='auto')\n",
        "history = model.fit(X_enc_train,y_train,\n",
        "                  epochs=20, batch_size=100,\n",
        "                  validation_split=0.2, callbacks=[esCallBack])\n",
        "\n",
        "results = model.evaluate(X_enc_test, y_test) \n",
        "print(\"Hasil  [loss,acc] untuk data test:\")\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 634 samples, validate on 159 samples\n",
            "Epoch 1/20\n",
            "634/634 [==============================] - 0s 108us/step - loss: 0.0104 - accuracy: 0.9984 - val_loss: 0.8801 - val_accuracy: 0.7987\n",
            "Epoch 2/20\n",
            "634/634 [==============================] - 0s 98us/step - loss: 0.0104 - accuracy: 0.9984 - val_loss: 0.8808 - val_accuracy: 0.7987\n",
            "Epoch 3/20\n",
            "634/634 [==============================] - 0s 90us/step - loss: 0.0104 - accuracy: 0.9984 - val_loss: 0.8816 - val_accuracy: 0.7987\n",
            "Epoch 4/20\n",
            "634/634 [==============================] - 0s 90us/step - loss: 0.0103 - accuracy: 0.9984 - val_loss: 0.8823 - val_accuracy: 0.7987\n",
            "199/199 [==============================] - 0s 76us/step\n",
            "Hasil  [loss,acc] untuk data test:\n",
            "[1.740022554469468, 0.6834170818328857]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i646ZF2JG2MC"
      },
      "source": [
        "**7. MENYIMPAN MODEL DAN HASIL TOKENIZER**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRZrCEzegYNA"
      },
      "source": [
        "import pickle\n",
        "model.save('model_spam_v1.h5')\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpMTmmH7G97J"
      },
      "source": [
        "**8. ME-LOAD MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHEA2fEzgn_P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "840cd3a4-3c07-45cf-c616-c7d49090da4b"
      },
      "source": [
        "model = load_model('model_spam_v1.h5')\n",
        "with open('tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "\n",
        "s  = [\"makanannya enak tempat nyaman.\", \"pelayanan buruk, harga sangat mahal, tempat tidak nyaman.\"]\n",
        "seq_str = tokenizer.texts_to_sequences(s)\n",
        "enc_str = tokenizer.sequences_to_matrix(seq_str,mode=\"tfidf\")\n",
        "enc_str.shape\n",
        "pred = model.predict_classes(enc_str)\n",
        "print(\"Prediksi kelas string ' {} ' \\nadalah {}\".format(s,pred))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediksi kelas string ' ['makanannya enak tempat nyaman.', 'pelayanan buruk, harga sangat mahal, tempat tidak nyaman.'] ' \n",
            "adalah [2 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYMB6NkoGXYj"
      },
      "source": [
        "**9. PERHITUNGAN AKURASI DAN F1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CR-FtRAibJx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "6efc96d3-b0d7-404e-8eff-b3e64b92d661"
      },
      "source": [
        "y_pred = model.predict_classes(X_enc_test)\n",
        "print(classification_report(np.argmax(y_test,axis=1), y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.81      0.66        59\n",
            "           1       0.44      0.11      0.18        35\n",
            "           2       0.83      0.82      0.82       105\n",
            "\n",
            "    accuracy                           0.69       199\n",
            "   macro avg       0.61      0.58      0.56       199\n",
            "weighted avg       0.68      0.69      0.66       199\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}